---
title: "HW2-STAT511"
author: "Jingyi Guo"
date: "2024-09-27"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

\section*{Question 1}

### Part 1

\begin{itemize}
    \item \textbf{Model A}:
    \[
    y_i = \beta_0 + \beta_1 z_i + \epsilon_i, \quad \text{where } z_i = 1 \text{ for women and } 0 \text{ for men and } \beta_0 = \theta_0, \beta_1 = \theta_1 - \theta_0. 
    \]
    Interpretation: $\beta_0$ is the mean response for men, and $\beta_0 + \beta_1$ is the mean response for women.
    
    \item \textbf{Model B}:
    \[
    y_i = \beta_0 + \beta_1 x_i + \epsilon_i, \quad \text{where } \beta_0 = \theta_0, \beta_1 = \theta_1.
    \]
    Interpretation: $\beta_0$ is the intercept, and $\beta_1$ is the effect of the continuous predictor $x$ (age).
    
    \item \textbf{Model C}:
    \[
    y_i = \beta_0 + \beta_1 x_i + \beta_2 z_i + \epsilon_i, \quad \text{where } \beta_0 = \theta_0, \beta_1 = \theta_1, \beta_2 = \theta_2 - \theta_1.
    \]
    Interpretation: $\beta_0$ is the intercept for men, and $\beta_0 + \beta_2$ is the intercept for women. $\beta_1$ is the effect of $x$ for both men and women.
    
    \item \textbf{Model D}:
    \[
    y_i = \beta_0 + \beta_1 x_i + \beta_2 z_i + \epsilon_i, \quad \text{where } \beta_0 = \theta_0, \beta_1 = \theta_1, \beta_2 = \phi_0.
    \]
    Interpretation: $\beta_0$ is the intercept for men, and $\beta_0 + \beta_2$ is the intercept for women. $\beta_1$ is the effect of $x$ for both genders.
    
    \item \textbf{Model E}:
    \[
    y_i = \beta_0 + \beta_1 x_i + \beta_2 z_i x_i + \epsilon_i, \quad \text{where } \beta_0 = \theta_0, \beta_1 = \theta_1, \beta_2 = \theta_2 - \theta_1.
    \]
    Interpretation: $\beta_0$ is the intercept, $\beta_1$ is the effect of $x$ for men, and $\beta_2$ modifies the effect of $x$ for women.
    
    \item \textbf{Model F}:
    \[
    y_i = \beta_0 + \beta_1 x_i + \beta_2 z_i + \beta_3 z_i x_i + \epsilon_i, \quad \text{where } \beta_0 = \theta_0, \beta_1 = \theta_1, \beta_2 = \theta_2 - \theta_0, \beta_3 = \theta_3 - \theta_1.
    \]
    Interpretation: $\beta_0$ is the intercept for men, $\beta_0 + \beta_2$ is the intercept for women, $\beta_1$ is the effect of $x$ for men, and $\beta_1 + \beta_3$ is the effect of $x$ for women.
\end{itemize}

### Part 2

Models D and C are equivalent because they have the same column space.

\section*{Question 2}

### Part 1

#### (a): Joint Distribution of $Y = (X_0, \dots, X_4) \in \mathbb{R}^5$

$$
Y = \begin{pmatrix}
X_0 \\
X_1 \\
X_2 \\
X_3 \\
X_4
\end{pmatrix}
=
\begin{pmatrix}
\epsilon_0 \\
\epsilon_1 + \theta_1 \epsilon_0 \\
\epsilon_2 + \theta_1 \epsilon_1 \\
\epsilon_3 + \theta_1 \epsilon_2 \\
\epsilon_4 + \theta_1 \epsilon_3
\end{pmatrix}
= 
\begin{pmatrix}
1 & 0 & 0 & 0 & 0 \\
\theta_1 & 1 & 0 & 0 & 0 \\
0 & \theta_1 & 1 & 0 & 0 \\
0 & 0 & \theta_1 & 1 & 0 \\
0 & 0 & 0 & \theta_1 & 1
\end{pmatrix}
\begin{pmatrix}
\epsilon_0 \\
\epsilon_1 \\
\epsilon_2 \\
\epsilon_3 \\
\epsilon_4
\end{pmatrix}
=
C \epsilon 
\quad \epsilon \sim N_5(0, \sigma_{\epsilon}^2 I_5)
$$

$$
\Rightarrow Y = C \epsilon \sim N_5\left(0, \sigma_{\epsilon}^2 C C^T\right)
= N_5\left(0, \Sigma
\right)
\text{ where }
\Sigma = \sigma_{\epsilon}^2
\begin{pmatrix}
1 & \theta_1 & 0 & 0 & 0 \\
\theta_1 & 1 + \theta_1^2 & \theta_1 & 0 & 0 \\
0 & \theta_1 & 1 + \theta_1^2 & \theta_1 & 0 \\
0 & 0 & \theta_1 & 1 + \theta_1^2 & \theta_1 \\
0 & 0 & 0 & \theta_1 & 1
\end{pmatrix}
$$

#### (b): Independence of $(X_1, X_3)$ and $(X_1, X_4)$


As $\Sigma_{13} = \Sigma_{14} = 0$, $(X_1, X_3)$ is independent, so is $(X_1, X_4)$

### Part 2

#### (a): Distribution of $z = (X'X)^{-1} X'y$:

$$
z \sim N_p(\beta, \sigma^2 (X'X)^{-1}).
$$
#### (b): Distribution of $\frac{1}{n} \sum_{i=1}^n Y_i$:

$$
\frac{1}{n} \sum_{i=1}^n Y_i \sim N\left( \frac{1}{n} \mathbb{1}^\top X \beta, \frac{\sigma^2}{n} \right).
$$

### Part 3

#### (a): The joint distribution of $b = [z', w']'$

$$
b \sim N_{n+m} \left( \begin{bmatrix} \mu 1_m \\ \gamma 1_n \end{bmatrix}, \begin{bmatrix} \sigma^2 I_m & 0 \\ 0 & R \end{bmatrix} \right).
$$
    
#### (b): The distribution of $(z - \mu 1_m)'(z - \mu 1_m)/\sigma^2$ 

Chi-squared with $m$ degrees of freedom, i.e., $\chi^2_m$.
    
#### (c): The distribution of $(z - \bar{z} 1_m)'(z - \bar{z} 1_m)/\sigma^2$
Chi-squared with $m-1$ degrees of freedom, i.e., $\chi^2_{m-1}$.
    
#### (d): The distribution of $(w - \gamma 1_n)' R^{-1} (w - \gamma 1_n)$

Chi-squared with $n$ degrees of freedom, i.e., $\chi^2_n$.
    
#### (e): The covariance $\text{Cov}(y_i, y_j)$

It is given by $R_{ij} = \exp(-|i - j|/10)$.

\section*{Question 3}

### Part 1

Load the provided HW2-quadratic.Rdata file into R. Use the lm() function to fit the quadratic model:

```{r}
load("HW2-quadratic.Rdata")
model <- lm(Y ~ poly(x1, 2) + poly(x2, 2) + x1*x2, data= dat)
summary(model)
```

the estimated function $f$ is:

$$
\hat{f}(x_1,x_2) = 1.02101x_1^2 + 6.81072x_2^2 + 0.04640x_1x_2 + 1.31436x_1 + -5.83353x_2 + 0.98357
$$

For the contour plot, use:

```{r}
library(ggplot2)

f <- function(x1, x2) {
  return(1.02101*x1^2 + 6.81072*x2^2 + 0.04640*x1*x2 + 1.31436*x1 - 5.83353*x2 + 0.98357)
}

x1 <- seq(-10, 10, length.out = 200)
x2 <- seq(-10, 10, length.out = 200)

grid <- expand.grid(x1 = x1, x2 = x2)
grid$z <- with(grid, f(x1, x2))

ggplot(grid, aes(x = x1, y = x2, z = z)) +
  geom_contour(bins = 15) +
  labs(title = "Contour Plot of Fitted Quadratic Model", x = "x1", y = "x2")
```


### Part 2

```{r}
library(car)
linearHypothesis(model, "x1:x2 = 0", singular.ok = TRUE)
```

Since the p-value is 0.659 (much larger than the typical significance level of 0.05), there is no significant evidence to reject the null hypothesis that there isn't any interaction between $x_1$ and $x_2$.


### Part 3

```{r}
linearHypothesis(model, c("poly(x1, 2)1","poly(x1, 2)2","x1:x2"), singular.ok = TRUE)
```

Since the p-value is greater than 0.05, we fail to reject the null hypothesis. This means that there is no significant evidence that $x_1$ and $Y$ are associated.


\section*{Question 4}

$$
L = \Pi_{t=1}^{100}\frac{(N_t\lambda_t)^{X_t}\exp\{-N_t\lambda_t\}}{X_t!} \\
l = \log{L} = \Sigma_{t=1}^{100} X_t\log(N_t\lambda_t) - \lambda_tN_t - \log(X_t!)
$$


```{r}
# Setup
set.seed(1234)
N_machines <- round(runif(n = 100,min = 800,max = 1000))

simulate_N_failures<- function(N_machines, n, rate0, rate1){
  stopifnot(n%%2==0)
  rate0_50 <- rep(rate0, length = n/2)
  rate1_50 <- rep(rate1, length = n/2)
  means <- c(rate0_50, rate1_50) * N_machines
  return(rpois(n = n, lambda = means))
}

set.seed(12345)
N_failures <- simulate_N_failures(N_machines = N_machines,n = 100, rate0 = 0.1,rate1 = 0.105)
```

### Part 1

$$
l = \Sigma_{t=1}^{100}[ X_t\log(N_t\lambda) - \lambda N_t - \log(X_t!)] \\
\frac{\partial l}{\partial \lambda} = \Sigma_{t=1}^{100} [\frac{X_t}{\lambda} - N_t] = 0 \Rightarrow \hat{\lambda} = \frac{\Sigma_{t=1}^{100} X_t}{\Sigma_{t=1}^{100} N_t}
$$

```{r}
# 1. MLE assuming no shift in rate
mle_no_shift <- sum(N_failures) / sum(N_machines)
mle_no_shift
```

the maximum likelihood estimate for $\lambda_t, t = 1, . . . , T$ is  0.1032047

### Part 2

```{r}
# 2. MLE assuming a rate shift between t = 50 and t = 51
mle_rate_0 <- sum(N_failures[1:50]) / sum(N_machines[1:50])
mle_rate_1 <- sum(N_failures[51:100]) / sum(N_machines[51:100])
mle_rate_0
mle_rate_1
```

The maximum likelihood estimate for $\lambda_t, t = 1, . . . , T/2$ is  0.1016983 and the maximum likelihood estimate for $\lambda_t, t = T/2+1, . . . , T$ is  0.1047327.

### Part 3

```{r}
# 3. Log-Likelihood Ratio Test
log_likelihood <- function(N_failures, N_machines, rate) {
  sum(dpois(N_failures, lambda = N_machines * rate, log = TRUE))
}

# Null hypothesis: No shift in rates
log_lik_null <- log_likelihood(N_failures, N_machines, mle_no_shift)

# Alternative hypothesis: Shift in rates at t = 50
log_lik_alt <- log_likelihood(N_failures[1:50], N_machines[1:50], mle_rate_0) +
               log_likelihood(N_failures[51:100], N_machines[51:100], mle_rate_1)

# Log-likelihood ratio test statistic
LLR_statistic <- -2 * (log_lik_null - log_lik_alt)
p_value_LLR <- pchisq(LLR_statistic, df = 1, lower.tail = FALSE)

# Wald Test
Wald_statistic <- (mle_rate_1 - mle_rate_0)^2 / (mle_rate_0 / sum(N_machines[1:50]) + mle_rate_1 / sum(N_machines[51:100]))
p_value_Wald <- pchisq(Wald_statistic, df = 1, lower.tail = FALSE)

# Score Test (Lagrange Multiplier Test)
score_statistic <- (sum(N_failures[51:100]) - sum(N_machines[51:100]) * mle_rate_0)^2 /
                   (sum(N_machines[51:100]) * mle_rate_0)
p_value_Score <- pchisq(score_statistic, df = 1, lower.tail = FALSE)

cat("p_value_LLR:",p_value_LLR,"\n")
cat("p_value_Wald:",p_value_Wald,"\n")
cat("p_value_Score:",p_value_Score,"\n")
```
### Part 4

```{r}
# 4. Simulate for B = 1000
set.seed(12345)
B <- 1000
type_I_errors <- numeric(B)
powers <- numeric(B)

for (b in 1:B) {
  # Case 1: rate0 = rate1 = 0.1 (Type I error)
  N_failures_b <- simulate_N_failures(N_machines = N_machines, n = 100, rate0 = 0.1, rate1 = 0.1)
  mle_no_shift_b <- sum(N_failures_b) / sum(N_machines)
  mle_rate_0_b <- sum(N_failures_b[1:50]) / sum(N_machines[1:50])
  mle_rate_1_b <- sum(N_failures_b[51:100]) / sum(N_machines[51:100])
  
  log_lik_null_b <- log_likelihood(N_failures_b, N_machines, mle_no_shift_b)
  log_lik_alt_b <- log_likelihood(N_failures_b[1:50], N_machines[1:50], mle_rate_0_b) +
                   log_likelihood(N_failures_b[51:100], N_machines[51:100], mle_rate_1_b)
  
  LLR_stat_b <- -2 * (log_lik_null_b - log_lik_alt_b)
  type_I_errors[b] <- (pchisq(LLR_stat_b, df = 1, lower.tail = FALSE) < 0.05)
  
  # Case 2: rate0 = 0.1, rate1 = 0.105 (Power)
  N_failures_b_power <- simulate_N_failures(N_machines = N_machines, n = 100, rate0 = 0.1, rate1 = 0.105)
  mle_no_shift_b_power <- sum(N_failures_b_power) / sum(N_machines)
  mle_rate_0_b_power <- sum(N_failures_b_power[1:50]) / sum(N_machines[1:50])
  mle_rate_1_b_power <- sum(N_failures_b_power[51:100]) / sum(N_machines[51:100])
  
  log_lik_null_b_power <- log_likelihood(N_failures_b_power, N_machines, mle_no_shift_b_power)
  log_lik_alt_b_power <- log_likelihood(N_failures_b_power[1:50], N_machines[1:50], mle_rate_0_b_power) +
                         log_likelihood(N_failures_b_power[51:100], N_machines[51:100], mle_rate_1_b_power)
  
  LLR_stat_b_power <- -2 * (log_lik_null_b_power - log_lik_alt_b_power)
  powers[b] <- (pchisq(LLR_stat_b_power, df = 1, lower.tail = FALSE) < 0.05)
}

# Empirical Type I Error Rate
type_I_error_rate <- mean(type_I_errors)

# Empirical Power
power_rate <- mean(powers)

# Output results
list(
  Type_I_Error_Rate = type_I_error_rate,
  Power = power_rate
)
```

\section*{Question 5}

```{r}
# Load necessary packages
library(ggplot2)
library(dplyr)

# Load mpg dataset
data("mpg")

# 1. Histogram for cty with overlaid density plot
ggplot(mpg, aes(x = cty)) +
  geom_histogram(aes(y = ..count..), bins = 20, fill = "blue", alpha = 0.6) +
  geom_density(aes(y = ..count..), color = "red", size = 1.2) +
  labs(x = "City Miles Per Gallon (cty)", y = "Count") +
  theme_minimal()

# 2. Barplot for class variable (car type)
ggplot(mpg, aes(x = class)) +
  geom_bar(fill = "lightblue") +
  labs(x = "Type of Car (class)", y = "Count") +
  theme_minimal()

# 3. Summary statistics for displ variable
summary_stats <- summary(mpg$displ)
IQR_displ <- IQR(mpg$displ)
range_displ <- range(mpg$displ)

cat("Summary Statistics for displ:\n")
print(summary_stats)
cat("Interquartile Range (IQR):", IQR_displ, "\n")
cat("Range:", range_displ[1], "-", range_displ[2], "\n")

# 4. Frequencies of each fuel type (fl)
fuel_freq <- table(mpg$fl)
cat("Frequencies of each fuel type (fl):\n")
print(fuel_freq)

# 5. Boxplots for cty by drive train type (drv), ordered by median
mpg$drv <- factor(mpg$drv, levels = c("f", "r", "4"))

ggplot(mpg, aes(x = drv, y = cty)) +
  geom_boxplot() +
  labs(x = "Drive Train Type (drv)", y = "City Miles Per Gallon (cty)") +
  theme_minimal()

# 6. Overlapping density plots for cty by drive train type
ggplot(mpg, aes(x = cty, fill = drv)) +
  geom_density(alpha = 0.4) +
  labs(x = "City Miles Per Gallon (cty)", y = "Density") +
  theme_minimal()

# 7. Line plot of average cty by drv, faceted by car class
avg_cty_by_drv_class <- mpg %>%
  group_by(drv, class) %>%
  summarise(avg_cty = mean(cty))

ggplot(avg_cty_by_drv_class, aes(x = drv, y = avg_cty, group = class, color = class)) +
  geom_line() +
  geom_point() +
  labs(x = "Drive Train Type (drv)", y = "Average City Miles Per Gallon (cty)") +
  theme_minimal()

# 8. Two-way table summarizing the relationship between type of car and drive train type
two_way_table <- table(mpg$class, mpg$drv)
cat("Two-way table for car type (class) and drive train (drv):\n")
print(two_way_table)

# 9. Scatter plot of engine displacement (displ) vs city miles per gallon (cty)
ggplot(mpg, aes(x = displ, y = cty)) +
  geom_point() +
  labs(x = "Engine Displacement (displ)", y = "City Miles Per Gallon (cty)") +
  theme_minimal()

# 10. Scatter plot after binning displ
mpg$displ_binned <- cut(mpg$displ, breaks = 4)

ggplot(mpg, aes(x = displ_binned, y = cty)) +
  geom_boxplot() +
  labs(x = "Binned Engine Displacement (displ)", y = "City Miles Per Gallon (cty)") +
  theme_minimal()

# 11. Scatter plot of displ vs cty colored by drv
ggplot(mpg, aes(x = displ, y = cty, color = drv)) +
  geom_point() +
  labs(x = "Engine Displacement (displ)", y = "City Miles Per Gallon (cty)") +
  theme_minimal()

```



